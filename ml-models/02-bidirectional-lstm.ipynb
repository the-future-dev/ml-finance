{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bidirectional LSTM\n",
    "\n",
    "Bidirectional Long Short-Term Memory (BiLSTM) networks are an extension of traditional Long Short-Term Memory (LSTM) networks. LSTMs are a type of recurrent neural network that is capable of learning long-term dependencies in data which is crucial for many tasks. However, standard LSTMs have a limitation of processing data in a single direction, from past to future, which may not always capture all the available information in the data. This is where Bidirectional LSTMs come into play. \n",
    "\n",
    "A Bidirectional LSTM consists of two LSTMs: one processing the data from past to future (as a standard LSTM) and another one processing the data from future to past. By doing this, BiLSTMs are able to preserve information from both past and future, providing a richer representation of data.\n",
    "\n",
    "## Pros and Cons of Bidirectional LSTM compared to standard LSTM\n",
    "\n",
    "### Pros:\n",
    "1. **Better Performance**: By accessing long-range information in both directions, BiLSTMs often outperform standard LSTMs, especially in tasks that benefit from context around each data point.\n",
    "2. **Richer Representations**: BiLSTMs can generate richer representations by capturing relationships in the data that may be missed by a unidirectional LSTM.\n",
    "3. **Improved Sequence Labelling**: In sequence labeling tasks, BiLSTMs have shown to perform significantly better as they have access to future context as well as past context.\n",
    "\n",
    "### Cons:\n",
    "1. **Increased Computational Complexity**: Due to the bidirectional nature, the training and inference times are roughly doubled compared to a standard LSTM.\n",
    "2. **Memory Usage**: BiLSTMs require more memory as they need to store intermediate states for forward and backward passes.\n",
    "3. **Potential Overfitting**: With more parameters to learn, BiLSTMs might be prone to overfitting especially on small datasets.\n",
    "\n",
    "In conclusion, BiLSTMs provide a powerful tool for tasks that can benefit from understanding data in both temporal directions. However, they come at the cost of increased computational and memory requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 16:57:18.746932: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-09 16:57:18.773457: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-09 16:57:18.773486: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-09 16:57:18.773502: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-09 16:57:18.778361: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-09 16:57:19.409094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>323.540009</td>\n",
       "      <td>324.890015</td>\n",
       "      <td>322.529999</td>\n",
       "      <td>324.869995</td>\n",
       "      <td>306.295227</td>\n",
       "      <td>59151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>321.160004</td>\n",
       "      <td>323.640015</td>\n",
       "      <td>321.100006</td>\n",
       "      <td>322.410004</td>\n",
       "      <td>303.975922</td>\n",
       "      <td>77709700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>320.489990</td>\n",
       "      <td>323.730011</td>\n",
       "      <td>320.359985</td>\n",
       "      <td>323.640015</td>\n",
       "      <td>305.135651</td>\n",
       "      <td>55653900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>323.019989</td>\n",
       "      <td>323.540009</td>\n",
       "      <td>322.239990</td>\n",
       "      <td>322.730011</td>\n",
       "      <td>304.277649</td>\n",
       "      <td>40496400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>322.940002</td>\n",
       "      <td>325.779999</td>\n",
       "      <td>322.670013</td>\n",
       "      <td>324.450012</td>\n",
       "      <td>305.899261</td>\n",
       "      <td>68296000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  2020-01-02  323.540009  324.890015  322.529999  324.869995  306.295227   \n",
       "1  2020-01-03  321.160004  323.640015  321.100006  322.410004  303.975922   \n",
       "2  2020-01-06  320.489990  323.730011  320.359985  323.640015  305.135651   \n",
       "3  2020-01-07  323.019989  323.540009  322.239990  322.730011  304.277649   \n",
       "4  2020-01-08  322.940002  325.779999  322.670013  324.450012  305.899261   \n",
       "\n",
       "     Volume  \n",
       "0  59151200  \n",
       "1  77709700  \n",
       "2  55653900  \n",
       "3  40496400  \n",
       "4  68296000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../ml-models/dataset/SPY_2020-01-01_2022-01-01.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:507: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  array.dtypes.apply(is_sparse).any()):\n",
      "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:507: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  array.dtypes.apply(is_sparse).any()):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.400424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.390759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.395592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.392017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.398774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.400424\n",
       "1  0.390759\n",
       "2  0.395592\n",
       "3  0.392017\n",
       "4  0.398774"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax = MinMaxScaler().fit(df.iloc[:, 4:5].astype('float32')) # Close index\n",
    "df_log = minmax.transform(df.iloc[:, 4:5].astype('float32')) # Close index\n",
    "df_log = pd.DataFrame(df_log)\n",
    "df_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and train the dataset\n",
    "The dataset will be splitted into training and test:\n",
    "1. Train dataset is derived from the starting timestamp until the last 30 days\n",
    "2. Test dataste is derived from the last 30 days of trading\n",
    "\n",
    "I will let the model do forecasting based on last 30 days, and we will repeat the experiment for 10 times.\n",
    "\n",
    "Try changing the tuning parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((505, 7), (490, 1), (15, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 15\n",
    "number_of_simulations = 1\n",
    "\n",
    "df_train = df_log.iloc[:-test_size]\n",
    "df_test = df_log.iloc[-test_size:]\n",
    "\n",
    "df.shape, df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "num_layers = 1\n",
    "size_layer = 128\n",
    "timestamp = 5\n",
    "epoch = 300\n",
    "dropout_rate = 0.8\n",
    "future_day = test_size\n",
    "learning_rate = 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Model(tf.keras.Model):\n",
    "    def __init__(self, num_layers, size_layer, output_size, dropout_rate):\n",
    "        super(BiLSTM_Model, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.size_layer = size_layer\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.bilstm_layers = [\n",
    "            tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.LSTM(self.size_layer, return_sequences=True),\n",
    "                merge_mode='concat'\n",
    "            )\n",
    "            for _ in range(self.num_layers)\n",
    "        ]\n",
    "        self.dense = tf.keras.layers.Dense(self.output_size)\n",
    "\n",
    "    def call(self, data, states=None):\n",
    "        x = data\n",
    "        for layer in self.bilstm_layers:\n",
    "            x = layer(x)\n",
    "        output = self.dense(x)\n",
    "        return output, states\n",
    "\n",
    "def forecast(df, df_log, df_train, learning_rate, num_layers, size_layer, dropout_rate, epoch, timestamp, test_size):\n",
    "    def anchor(signal, weight):\n",
    "        buffer = []\n",
    "        last = signal[0]\n",
    "        for i in signal:\n",
    "            smoothed_val = last * weight + (1 - weight) * i\n",
    "            buffer.append(smoothed_val)\n",
    "            last = smoothed_val\n",
    "        return buffer\n",
    "    \n",
    "    modelnn = BiLSTM_Model(num_layers, size_layer, df_log.shape[1], dropout_rate)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()\n",
    "\n",
    "    for i in tqdm(range(epoch), desc='train loop'):\n",
    "        total_loss = []\n",
    "        for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0])\n",
    "            batch_x = np.expand_dims(df_train.iloc[k:index, :].values, axis=0)\n",
    "            batch_y = df_train.iloc[k + 1:index + 1, :].values\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits, _ = modelnn(batch_x, states=None)\n",
    "                loss = tf.reduce_mean(tf.square(batch_y - logits))\n",
    "                total_loss.append(loss.numpy())\n",
    "            \n",
    "            grads = tape.gradient(loss, modelnn.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, modelnn.trainable_variables))\n",
    "        \n",
    "        print(f'Epoch {i}, Loss: {np.mean(total_loss)}')\n",
    "    \n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, _ = modelnn(\n",
    "            np.expand_dims(df_train.iloc[k:k + timestamp], axis=0),\n",
    "            states=None\n",
    "        )\n",
    "        output_predict[k + 1:k + timestamp + 1] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits, _ = modelnn(\n",
    "            np.expand_dims(df_train.iloc[upper_b:], axis = 0),\n",
    "            states=None\n",
    "        )\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "        date_ori.append(date_ori[-1] + timedelta(days = 1))\n",
    "\n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        out_logits, _ = modelnn(\n",
    "            np.expand_dims(o, axis = 0),\n",
    "            states=None\n",
    "        )\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "        date_ori.append(date_ori[-1] + timedelta(days = 1))\n",
    "\n",
    "    # Assuming you have a MinMaxScaler object named minmax to reverse the scaling\n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    \n",
    "    # Assuming anchor function is defined elsewhere to process the output\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    return deep_future[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loop:   0%|          | 0/300 [00:00<?, ?it/s]2023-10-09 17:12:12.114276: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:521] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  /usr/local/cuda-11.8\n",
      "  /usr/local/cuda\n",
      "  /home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "2023-10-09 17:12:12.121003: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-10-09 17:12:12.121982: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-10-09 17:12:12.122008: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2023-10-09 17:12:12.122715: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-10-09 17:12:12.122751: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-10-09 17:12:12.170705: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-10-09 17:12:12.170764: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2023-10-09 17:12:12.171550: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-10-09 17:12:12.171587: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-10-09 17:12:12.479402: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:435] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "train loop:   0%|          | 0/300 [00:00<?, ?it/s]\n",
      "2023-10-09 17:12:12.480102: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at cudnn_rnn_ops.cc:1764 : UNKNOWN: Fail to find the dnn implementation.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Exception encountered when calling layer 'forward_lstm_1' (type LSTM).\n\n{{function_node __wrapped__CudnnRNN_device_/job:localhost/replica:0/task:0/device:GPU:0}} Fail to find the dnn implementation. [Op:CudnnRNN]\n\nCall arguments received by layer 'forward_lstm_1' (type LSTM):\n  • inputs=tf.Tensor(shape=(1, 5, 1), dtype=float32)\n  • mask=None\n  • training=None\n  • initial_state=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/andrea/Desktop/ml-finance/ml-models/02-bidirectional-lstm.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/02-bidirectional-lstm.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m deep_future \u001b[39m=\u001b[39m forecast(df, df_log, df_train, learning_rate, num_layers, size_layer, dropout_rate, epoch, timestamp, test_size)\n",
      "\u001b[1;32m/home/andrea/Desktop/ml-finance/ml-models/02-bidirectional-lstm.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/02-bidirectional-lstm.ipynb#X15sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m batch_y \u001b[39m=\u001b[39m df_train\u001b[39m.\u001b[39miloc[k \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m:index \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/02-bidirectional-lstm.ipynb#X15sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/02-bidirectional-lstm.ipynb#X15sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     logits, _ \u001b[39m=\u001b[39m modelnn(batch_x, states\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/02-bidirectional-lstm.ipynb#X15sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(tf\u001b[39m.\u001b[39msquare(batch_y \u001b[39m-\u001b[39m logits))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/02-bidirectional-lstm.ipynb#X15sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     total_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;32m/home/andrea/Desktop/ml-finance/ml-models/02-bidirectional-lstm.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/02-bidirectional-lstm.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m x \u001b[39m=\u001b[39m data\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/02-bidirectional-lstm.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbilstm_layers:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/02-bidirectional-lstm.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/02-bidirectional-lstm.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/02-bidirectional-lstm.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output, states\n",
      "\u001b[0;31mUnknownError\u001b[0m: Exception encountered when calling layer 'forward_lstm_1' (type LSTM).\n\n{{function_node __wrapped__CudnnRNN_device_/job:localhost/replica:0/task:0/device:GPU:0}} Fail to find the dnn implementation. [Op:CudnnRNN]\n\nCall arguments received by layer 'forward_lstm_1' (type LSTM):\n  • inputs=tf.Tensor(shape=(1, 5, 1), dtype=float32)\n  • mask=None\n  • training=None\n  • initial_state=None"
     ]
    }
   ],
   "source": [
    "deep_future = forecast(df, df_log, df_train, learning_rate, num_layers, size_layer, dropout_rate, epoch, timestamp, test_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
