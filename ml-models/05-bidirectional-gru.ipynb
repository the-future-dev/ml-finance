{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 17:30:35.579743: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-09 17:30:35.607331: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-09 17:30:35.607359: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-09 17:30:35.607376: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-09 17:30:35.611966: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-09 17:30:36.277411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Bidirectional\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "sns.set()\n",
    "tf.compat.v1.random.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:507: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  array.dtypes.apply(is_sparse).any()):\n",
      "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:507: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  array.dtypes.apply(is_sparse).any()):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((505, 7), (475, 1), (30, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../ml-models/dataset/SPY_2020-01-01_2022-01-01.csv')\n",
    "\n",
    "# Adj Close Index= 5:6\n",
    "minmax = MinMaxScaler().fit(df.iloc[:, 5:6].astype('float32'))\n",
    "df_log = minmax.transform(df.iloc[:, 5:6].astype('float32'))\n",
    "df_log = pd.DataFrame(df_log)\n",
    "\n",
    "test_size = 30\n",
    "simulation_size = 10\n",
    "\n",
    "df_train = df_log.iloc[:-test_size]\n",
    "df_test = df_log.iloc[-test_size:]\n",
    "df.shape, df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "size_layer = 128\n",
    "timestamp = 1\n",
    "epoch = 300\n",
    "dropout_rate = 0.8\n",
    "future_day = test_size\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias=0.1,\n",
    "    ):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.gru_layers_forward = [GRU(size_layer, return_sequences=True, return_state=True) for _ in range(num_layers)]\n",
    "        self.gru_layers_backward = [GRU(size_layer, return_sequences=True, return_state=True, go_backwards=True) for _ in range(num_layers)]\n",
    "        self.dense = Dense(output_size)\n",
    "        self.optimizer_instance = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x, initial_state_fw, initial_state_bw = inputs\n",
    "        states_fw = initial_state_fw\n",
    "        states_bw = initial_state_bw\n",
    "        for layer_fw, layer_bw in zip(self.gru_layers_forward, self.gru_layers_backward):\n",
    "            x_fw, states_fw = layer_fw(x, initial_state=states_fw)\n",
    "            x_bw, states_bw = layer_bw(x, initial_state=states_bw)\n",
    "        x = tf.concat([x_fw, x_bw], axis=-1)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y, initial_state_fw, initial_state_bw = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self((x, initial_state_fw, initial_state_bw), training=True)\n",
    "            loss = tf.reduce_mean(tf.square(y - logits))\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer_instance.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return {'loss': loss}\n",
    "\n",
    "def calculate_accuracy(real, predict):\n",
    "    real = np.array(real) + 1\n",
    "    predict = np.array(predict) + 1\n",
    "    percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))\n",
    "    return percentage * 100\n",
    "\n",
    "def anchor(signal, weight):\n",
    "    buffer = []\n",
    "    last = signal[0]\n",
    "    for i in signal:\n",
    "        smoothed_val = last * weight + (1 - weight) * i\n",
    "        buffer.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast():\n",
    "    modelnn = CustomModel(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "\n",
    "    modelnn.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss='mean_squared_error'  # Assuming you're using Mean Squared Error loss\n",
    "    )\n",
    "\n",
    "    date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()\n",
    "\n",
    "    pbar = tqdm(range(epoch), desc='train loop')\n",
    "    for i in pbar:\n",
    "        init_value_forward = np.zeros((1, num_layers * size_layer))\n",
    "        init_value_backward = np.zeros((1, num_layers * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            history = modelnn.train_on_batch(\n",
    "                [batch_x, init_value_forward, init_value_backward],\n",
    "                batch_y\n",
    "            )\n",
    "            logits = modelnn.predict_on_batch([batch_x, init_value_forward, init_value_backward])\n",
    "            loss = history['loss']\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "        pbar.set_postfix(cost=np.mean(total_loss), acc=np.mean(total_acc))\n",
    "    \n",
    "\n",
    "    future_day = test_size\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value_forward = np.zeros((1, num_layers * size_layer))\n",
    "    init_value_backward = np.zeros((1, num_layers * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits = modelnn.predict_on_batch(\n",
    "            [np.expand_dims(df_train.iloc[k: k + timestamp], axis=0),\n",
    "             init_value_forward, init_value_backward]\n",
    "        )\n",
    "        # Assume that modelnn.call or modelnn.predict_on_batch returns logits and state as a tuple\n",
    "        init_value_forward, init_value_backward = out_logits[1]\n",
    "        output_predict[k + 1: k + timestamp + 1] = out_logits[0]\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits = modelnn.predict_on_batch(\n",
    "            [np.expand_dims(df_train.iloc[upper_b:], axis=0),\n",
    "             init_value_forward, init_value_backward]\n",
    "        )\n",
    "        init_value_forward, init_value_backward = out_logits[1]\n",
    "        output_predict[upper_b + 1: df_train.shape[0] + 1] = out_logits[0]\n",
    "        future_day -= 1\n",
    "        date_ori.append(date_ori[-1] + timedelta(days=1))\n",
    "\n",
    "    init_value_forward = out_logits[1][0]\n",
    "    init_value_backward = out_logits[1][1]\n",
    "\n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        out_logits = modelnn.predict_on_batch(\n",
    "            [np.expand_dims(o, axis=0),\n",
    "             init_value_forward, init_value_backward]\n",
    "        )\n",
    "        init_value_forward, init_value_backward = out_logits[1]\n",
    "        output_predict[-future_day + i] = out_logits[0][-1]\n",
    "        date_ori.append(date_ori[-1] + timedelta(days=1))\n",
    "\n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "\n",
    "    return deep_future[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 17:30:53.228473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-09 17:30:53.236137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-09 17:30:53.236319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-09 17:30:53.237322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-09 17:30:53.237468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-09 17:30:53.237574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-09 17:30:53.526648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-09 17:30:53.526787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-09 17:30:53.526889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-09 17:30:53.526978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1531 MB memory:  -> device: 0, name: NVIDIA GeForce MX450, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "train loop:   0%|          | 0/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/tmp/ipykernel_111320/1922925663.py\", line 29, in train_step\n        x, y, initial_state_fw, initial_state_bw = data\n\n    ValueError: not enough values to unpack (expected 4, got 2)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msimulation \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     results\u001b[39m.\u001b[39mappend(forecast())\n",
      "\u001b[1;32m/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m batch_x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     df_train\u001b[39m.\u001b[39miloc[k : index, :]\u001b[39m.\u001b[39mvalues, axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m batch_y \u001b[39m=\u001b[39m df_train\u001b[39m.\u001b[39miloc[k \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m : index \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m history \u001b[39m=\u001b[39m modelnn\u001b[39m.\u001b[39;49mtrain_on_batch(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     [batch_x, init_value_forward, init_value_backward],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     batch_y\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m logits \u001b[39m=\u001b[39m modelnn\u001b[39m.\u001b[39mpredict_on_batch([batch_x, init_value_forward, init_value_backward])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m loss \u001b[39m=\u001b[39m history[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:2763\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2759\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39msingle_batch_iterator(\n\u001b[1;32m   2760\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[1;32m   2761\u001b[0m     )\n\u001b[1;32m   2762\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[0;32m-> 2763\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   2765\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2766\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileisxz7fq6.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:1360\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[1;32m   1357\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m     )\n\u001b[1;32m   1359\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1360\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[1;32m   1361\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1362\u001b[0m     outputs,\n\u001b[1;32m   1363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[1;32m   1364\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m   1365\u001b[0m )\n\u001b[1;32m   1366\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:1349\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1349\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m   1350\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "\u001b[1;32m/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     x, y, initial_state_fw, initial_state_bw \u001b[39m=\u001b[39m data\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/ml-models/05-bidirectional-gru.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m((x, initial_state_fw, initial_state_bw), training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/tmp/ipykernel_111320/1922925663.py\", line 29, in train_step\n        x, y, initial_state_fw, initial_state_bw = data\n\n    ValueError: not enough values to unpack (expected 4, got 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "results = []\n",
    "for i in range(1):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    results.append(forecast())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
