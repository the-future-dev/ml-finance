{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 21:07:12.510679: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-05 21:07:12.541211: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-05 21:07:12.541244: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-05 21:07:12.541262: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-05 21:07:12.546639: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-05 21:07:13.161865: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dense, Flatten, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from shared import read_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data:<br>\n",
    "dall'SP500 prendiamo `Open`, `Close`, `High`, `Low`, `Volume`\n",
    "\n",
    "Target:\n",
    "- `1` if the close price difference is over 1.5%\n",
    "- `0` otherwise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(753, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>pct change</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>2022.150024</td>\n",
       "      <td>2030.250000</td>\n",
       "      <td>1992.439941</td>\n",
       "      <td>2002.609985</td>\n",
       "      <td>4460110000</td>\n",
       "      <td>-0.008893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>2005.550049</td>\n",
       "      <td>2029.609985</td>\n",
       "      <td>2005.550049</td>\n",
       "      <td>2025.900024</td>\n",
       "      <td>3805480000</td>\n",
       "      <td>0.011630</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>2030.609985</td>\n",
       "      <td>2064.080078</td>\n",
       "      <td>2030.609985</td>\n",
       "      <td>2062.139893</td>\n",
       "      <td>3934010000</td>\n",
       "      <td>0.017888</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-09</th>\n",
       "      <td>2063.449951</td>\n",
       "      <td>2064.429932</td>\n",
       "      <td>2038.329956</td>\n",
       "      <td>2044.810059</td>\n",
       "      <td>3364140000</td>\n",
       "      <td>-0.008404</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-12</th>\n",
       "      <td>2046.130005</td>\n",
       "      <td>2049.300049</td>\n",
       "      <td>2022.579956</td>\n",
       "      <td>2028.260010</td>\n",
       "      <td>3456460000</td>\n",
       "      <td>-0.008094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close      Volume  \\\n",
       "Date                                                                         \n",
       "2015-01-06  2022.150024  2030.250000  1992.439941  2002.609985  4460110000   \n",
       "2015-01-07  2005.550049  2029.609985  2005.550049  2025.900024  3805480000   \n",
       "2015-01-08  2030.609985  2064.080078  2030.609985  2062.139893  3934010000   \n",
       "2015-01-09  2063.449951  2064.429932  2038.329956  2044.810059  3364140000   \n",
       "2015-01-12  2046.130005  2049.300049  2022.579956  2028.260010  3456460000   \n",
       "\n",
       "            pct change  target  \n",
       "Date                            \n",
       "2015-01-06   -0.008893       0  \n",
       "2015-01-07    0.011630       1  \n",
       "2015-01-08    0.017888       1  \n",
       "2015-01-09   -0.008404       0  \n",
       "2015-01-12   -0.008094       0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = \"2015-01-05\"\n",
    "end = \"2017-12-29\"\n",
    "\n",
    "SP500 = read_dataset('../data/^GSPC.csv', start, end)\n",
    "SP500['pct change'] = SP500['Close'].pct_change()\n",
    "SP500.dropna(inplace=True)\n",
    "\n",
    "SP500['target'] = SP500['pct change'].apply(lambda x: 1 if x > 0 else 0)\n",
    "SP500 = SP500.drop('Adj Close', axis=1)\n",
    "\n",
    "print(SP500.shape)\n",
    "\n",
    "SP500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: (602, 20, 7) train_target: (602, 2) | y class: (602,)\n",
      "test_data: (131, 20, 7) test_target: (131, 2) | y class: (131,)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(SP500.values)\n",
    "target = np.array(SP500['target'].values)\n",
    "\n",
    "# scaler_data = MinMaxScaler()\n",
    "# scaler_data.fit(data)\n",
    "# data = scaler_data.transform(data)\n",
    "\n",
    "train_size = int(len(data) * 0.80)\n",
    "\n",
    "n_features = data.shape[-1]\n",
    "n_timesteps = 20\n",
    "def create_sequences(data, target, n_timesteps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_timesteps):\n",
    "        X.append(data[i:i+n_timesteps])\n",
    "        y.append(target[i+n_timesteps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "data_seq, target_seq = create_sequences(data, target, n_timesteps)\n",
    "\n",
    "x_train, x_test = np.array(data_seq[:train_size]), np.array(data_seq[train_size:])\n",
    "y_train_class, y_test_class = np.array(target_seq[:train_size]), np.array(target_seq[train_size:])\n",
    "\n",
    "y_train = np.eye(2)[y_train_class]\n",
    "y_test = np.eye(2)[y_test_class]\n",
    "\n",
    "print(f'train_data: {x_train.shape} train_target: {y_train.shape} | y class: {y_train_class.shape}')\n",
    "print(f'test_data: {x_test.shape} test_target: {y_test.shape} | y class: {y_test_class.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train [16]:\n",
      "[ 9.53468529e-03 -1.29919653e-02  1.29624637e-02  1.44394953e-02\n",
      " -4.15604621e-03  1.02914067e-02 -3.41817231e-03 -4.24719436e-03\n",
      "  1.06755610e-02 -2.90335887e-05  9.64450643e-03  4.07473849e-03\n",
      "  1.59757446e-03 -3.14309218e-04 -1.06205733e-03  6.12653363e-03\n",
      " -3.03339195e-04  2.75877089e-03 -7.65723648e-04 -1.47602823e-03]\n",
      "[1. 0.]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "index = 16\n",
    "column = 5\n",
    "print(f'Train [{index}]:')\n",
    "print(x_train[index][:,column])\n",
    "print(y_train[index])\n",
    "print(y_train_class[index])\n",
    "# print(w_[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, l2_value):\n",
    "    kernel_size = 3\n",
    "    pool_size = 1\n",
    "    l2_ = l2(l2_value)\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Conv1D(32, kernel_size=kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(64, kernel_size, activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2_, bias_regularizer=l2_))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(128, kernel_size, activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2_, bias_regularizer=l2_))\n",
    "    # model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    # model.add(Conv1D(256, kernel_size, activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2_, bias_regularizer=l2_))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(256, kernel_size, activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2_, bias_regularizer=l2_))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # model.add(Dense(256, activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2_, bias_regularizer=l2_))\n",
    "    model.add(Dense(256, activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2_, bias_regularizer=l2_))\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 18009474.0000 - recall_7: 0.5052 - accuracy: 0.5052 - precision_7: 0.5052 - val_loss: 574615296.0000 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 654315008.0000 - recall_7: 0.4990 - accuracy: 0.4990 - precision_7: 0.4990 - val_loss: 262238688.0000 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 298629792.0000 - recall_7: 0.4990 - accuracy: 0.4990 - precision_7: 0.4990 - val_loss: 126784776.0000 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 138607584.0000 - recall_7: 0.4990 - accuracy: 0.4990 - precision_7: 0.4990 - val_loss: 105793000.0000 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 108778048.0000 - recall_7: 0.5010 - accuracy: 0.5010 - precision_7: 0.5010 - val_loss: 98283640.0000 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 103599376.0000 - recall_7: 0.5010 - accuracy: 0.5010 - precision_7: 0.5010 - val_loss: 43631264.0000 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 45879520.0000 - recall_7: 0.5010 - accuracy: 0.5010 - precision_7: 0.5010 - val_loss: 7806854.5000 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7872087.0000 - recall_7: 0.5031 - accuracy: 0.5031 - precision_7: 0.5031 - val_loss: 19622644.0000 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 22025520.0000 - recall_7: 0.4990 - accuracy: 0.4990 - precision_7: 0.4990 - val_loss: 23974178.0000 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 27530008.0000 - recall_7: 0.4990 - accuracy: 0.4990 - precision_7: 0.4990 - val_loss: 16798446.0000 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 19248142.0000 - recall_7: 0.4990 - accuracy: 0.4990 - precision_7: 0.4990 - val_loss: 9043684.0000 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 9985210.0000 - recall_7: 0.4990 - accuracy: 0.4990 - precision_7: 0.4990 - val_loss: 2176856.5000 - val_recall_7: 0.4545 - val_accuracy: 0.4545 - val_precision_7: 0.4545\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2210013.0000 - recall_7: 0.5385 - accuracy: 0.5385 - precision_7: 0.5385 - val_loss: 9069727.0000 - val_recall_7: 0.4628 - val_accuracy: 0.4628 - val_precision_7: 0.4628\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 9521036.0000 - recall_7: 0.5073 - accuracy: 0.5073 - precision_7: 0.5073 - val_loss: 8055661.0000 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8360082.0000 - recall_7: 0.5031 - accuracy: 0.5031 - precision_7: 0.5031 - val_loss: 7802170.5000 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7802596.5000 - recall_7: 0.5010 - accuracy: 0.5010 - precision_7: 0.5010 - val_loss: 6329903.0000 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6272154.0000 - recall_7: 0.5010 - accuracy: 0.5010 - precision_7: 0.5010 - val_loss: 5169322.5000 - val_recall_7: 0.4545 - val_accuracy: 0.4545 - val_precision_7: 0.4545\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5358340.5000 - recall_7: 0.5198 - accuracy: 0.5198 - precision_7: 0.5198 - val_loss: 3824878.2500 - val_recall_7: 0.5372 - val_accuracy: 0.5372 - val_precision_7: 0.5372\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4137632.2500 - recall_7: 0.4906 - accuracy: 0.4906 - precision_7: 0.4906 - val_loss: 4576588.5000 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5231850.0000 - recall_7: 0.4990 - accuracy: 0.4990 - precision_7: 0.4990 - val_loss: 3953532.5000 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4537124.0000 - recall_7: 0.4990 - accuracy: 0.4990 - precision_7: 0.4990 - val_loss: 3821952.7500 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4124764.7500 - recall_7: 0.4990 - accuracy: 0.4990 - precision_7: 0.4990 - val_loss: 3517961.5000 - val_recall_7: 0.4793 - val_accuracy: 0.4793 - val_precision_7: 0.4793\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3679799.0000 - recall_7: 0.5696 - accuracy: 0.5696 - precision_7: 0.5696 - val_loss: 2511287.0000 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2526645.0000 - recall_7: 0.5114 - accuracy: 0.5114 - precision_7: 0.5114 - val_loss: 2751394.5000 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2702545.5000 - recall_7: 0.5073 - accuracy: 0.5073 - precision_7: 0.5073 - val_loss: 2110191.0000 - val_recall_7: 0.4628 - val_accuracy: 0.4628 - val_precision_7: 0.4628\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2089031.6250 - recall_7: 0.5135 - accuracy: 0.5135 - precision_7: 0.5135 - val_loss: 2389763.5000 - val_recall_7: 0.4463 - val_accuracy: 0.4463 - val_precision_7: 0.4463\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2451566.7500 - recall_7: 0.5468 - accuracy: 0.5468 - precision_7: 0.5468 - val_loss: 1521030.6250 - val_recall_7: 0.5537 - val_accuracy: 0.5537 - val_precision_7: 0.5537\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1676228.7500 - recall_7: 0.4990 - accuracy: 0.4990 - precision_7: 0.4990 - val_loss: 1611537.0000 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1776174.6250 - recall_7: 0.5010 - accuracy: 0.5010 - precision_7: 0.5010 - val_loss: 1734660.3750 - val_recall_7: 0.5372 - val_accuracy: 0.5372 - val_precision_7: 0.5372\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1926106.7500 - recall_7: 0.4969 - accuracy: 0.4969 - precision_7: 0.4969 - val_loss: 2051060.5000 - val_recall_7: 0.5372 - val_accuracy: 0.5372 - val_precision_7: 0.5372\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2198281.0000 - recall_7: 0.5031 - accuracy: 0.5031 - precision_7: 0.5031 - val_loss: 1374911.3750 - val_recall_7: 0.5041 - val_accuracy: 0.5041 - val_precision_7: 0.5041\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1452054.7500 - recall_7: 0.5364 - accuracy: 0.5364 - precision_7: 0.5364 - val_loss: 702723.6875 - val_recall_7: 0.4380 - val_accuracy: 0.4380 - val_precision_7: 0.4380\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 719753.5000 - recall_7: 0.5634 - accuracy: 0.5634 - precision_7: 0.5634 - val_loss: 1469192.1250 - val_recall_7: 0.4463 - val_accuracy: 0.4463 - val_precision_7: 0.4463\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1453753.7500 - recall_7: 0.5738 - accuracy: 0.5738 - precision_7: 0.5738 - val_loss: 1618072.7500 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1624945.5000 - recall_7: 0.5800 - accuracy: 0.5800 - precision_7: 0.5800 - val_loss: 889939.8125 - val_recall_7: 0.4959 - val_accuracy: 0.4959 - val_precision_7: 0.4959\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 871735.4375 - recall_7: 0.5696 - accuracy: 0.5696 - precision_7: 0.5696 - val_loss: 616342.8750 - val_recall_7: 0.5372 - val_accuracy: 0.5372 - val_precision_7: 0.5372\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 644566.0625 - recall_7: 0.5447 - accuracy: 0.5447 - precision_7: 0.5447 - val_loss: 1117143.8750 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1157711.1250 - recall_7: 0.5572 - accuracy: 0.5572 - precision_7: 0.5572 - val_loss: 873859.4375 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 909938.1875 - recall_7: 0.5551 - accuracy: 0.5551 - precision_7: 0.5551 - val_loss: 374525.2500 - val_recall_7: 0.5537 - val_accuracy: 0.5537 - val_precision_7: 0.5537\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 383895.7500 - recall_7: 0.5509 - accuracy: 0.5509 - precision_7: 0.5509 - val_loss: 840246.1875 - val_recall_7: 0.5372 - val_accuracy: 0.5372 - val_precision_7: 0.5372\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 810804.2500 - recall_7: 0.5676 - accuracy: 0.5676 - precision_7: 0.5676 - val_loss: 847423.3750 - val_recall_7: 0.4628 - val_accuracy: 0.4628 - val_precision_7: 0.4628\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 802561.2500 - recall_7: 0.5780 - accuracy: 0.5780 - precision_7: 0.5780 - val_loss: 484153.1875 - val_recall_7: 0.4463 - val_accuracy: 0.4463 - val_precision_7: 0.4463\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 453381.7812 - recall_7: 0.5759 - accuracy: 0.5759 - precision_7: 0.5759 - val_loss: 591012.1875 - val_recall_7: 0.5702 - val_accuracy: 0.5702 - val_precision_7: 0.5702\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 584672.1250 - recall_7: 0.5800 - accuracy: 0.5800 - precision_7: 0.5800 - val_loss: 612217.4375 - val_recall_7: 0.5455 - val_accuracy: 0.5455 - val_precision_7: 0.5455\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 627351.1875 - recall_7: 0.5593 - accuracy: 0.5593 - precision_7: 0.5593 - val_loss: 426425.7812 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 413263.7812 - recall_7: 0.5530 - accuracy: 0.5530 - precision_7: 0.5530 - val_loss: 536772.9375 - val_recall_7: 0.5620 - val_accuracy: 0.5620 - val_precision_7: 0.5620\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 485853.1875 - recall_7: 0.5800 - accuracy: 0.5800 - precision_7: 0.5800 - val_loss: 567061.1875 - val_recall_7: 0.4463 - val_accuracy: 0.4463 - val_precision_7: 0.4463\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 502236.6875 - recall_7: 0.5613 - accuracy: 0.5613 - precision_7: 0.5613 - val_loss: 471866.3125 - val_recall_7: 0.4298 - val_accuracy: 0.4298 - val_precision_7: 0.4298\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 411297.6562 - recall_7: 0.5551 - accuracy: 0.5551 - precision_7: 0.5551 - val_loss: 414868.7188 - val_recall_7: 0.5372 - val_accuracy: 0.5372 - val_precision_7: 0.5372\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 382752.1875 - recall_7: 0.5904 - accuracy: 0.5904 - precision_7: 0.5904 - val_loss: 427968.4375 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 404988.5312 - recall_7: 0.5364 - accuracy: 0.5364 - precision_7: 0.5364 - val_loss: 330852.9062 - val_recall_7: 0.5455 - val_accuracy: 0.5455 - val_precision_7: 0.5455\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 295697.5000 - recall_7: 0.5759 - accuracy: 0.5759 - precision_7: 0.5759 - val_loss: 390756.5312 - val_recall_7: 0.4215 - val_accuracy: 0.4215 - val_precision_7: 0.4215\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 314372.8438 - recall_7: 0.5800 - accuracy: 0.5800 - precision_7: 0.5800 - val_loss: 463254.4688 - val_recall_7: 0.4380 - val_accuracy: 0.4380 - val_precision_7: 0.4380\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 383615.8750 - recall_7: 0.5551 - accuracy: 0.5551 - precision_7: 0.5551 - val_loss: 272341.4375 - val_recall_7: 0.4463 - val_accuracy: 0.4463 - val_precision_7: 0.4463\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 215105.0469 - recall_7: 0.6133 - accuracy: 0.6133 - precision_7: 0.6133 - val_loss: 376086.3438 - val_recall_7: 0.5207 - val_accuracy: 0.5207 - val_precision_7: 0.5207\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 334968.6562 - recall_7: 0.5364 - accuracy: 0.5364 - precision_7: 0.5364 - val_loss: 310200.6875 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 260219.1719 - recall_7: 0.5800 - accuracy: 0.5800 - precision_7: 0.5800 - val_loss: 312952.7500 - val_recall_7: 0.4298 - val_accuracy: 0.4298 - val_precision_7: 0.4298\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 233513.3438 - recall_7: 0.5884 - accuracy: 0.5884 - precision_7: 0.5884 - val_loss: 365193.3125 - val_recall_7: 0.4298 - val_accuracy: 0.4298 - val_precision_7: 0.4298\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 282148.1562 - recall_7: 0.5676 - accuracy: 0.5676 - precision_7: 0.5676 - val_loss: 206425.5469 - val_recall_7: 0.4876 - val_accuracy: 0.4876 - val_precision_7: 0.4876\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 146747.7656 - recall_7: 0.6175 - accuracy: 0.6175 - precision_7: 0.6175 - val_loss: 299928.3125 - val_recall_7: 0.5207 - val_accuracy: 0.5207 - val_precision_7: 0.5207\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 247348.3906 - recall_7: 0.5572 - accuracy: 0.5572 - precision_7: 0.5572 - val_loss: 198944.7812 - val_recall_7: 0.5289 - val_accuracy: 0.5289 - val_precision_7: 0.5289\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 137076.0469 - recall_7: 0.6362 - accuracy: 0.6362 - precision_7: 0.6362 - val_loss: 309842.0625 - val_recall_7: 0.4215 - val_accuracy: 0.4215 - val_precision_7: 0.4215\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 224865.4844 - recall_7: 0.5821 - accuracy: 0.5821 - precision_7: 0.5821 - val_loss: 238069.0156 - val_recall_7: 0.4628 - val_accuracy: 0.4628 - val_precision_7: 0.4628\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 158057.0625 - recall_7: 0.6237 - accuracy: 0.6237 - precision_7: 0.6237 - val_loss: 237887.1406 - val_recall_7: 0.5124 - val_accuracy: 0.5124 - val_precision_7: 0.5124\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 178879.7031 - recall_7: 0.5988 - accuracy: 0.5988 - precision_7: 0.5988 - val_loss: 217585.3438 - val_recall_7: 0.4876 - val_accuracy: 0.4876 - val_precision_7: 0.4876\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 158466.8125 - recall_7: 0.6237 - accuracy: 0.6237 - precision_7: 0.6237 - val_loss: 234419.2656 - val_recall_7: 0.4380 - val_accuracy: 0.4380 - val_precision_7: 0.4380\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 149291.4062 - recall_7: 0.6154 - accuracy: 0.6154 - precision_7: 0.6154 - val_loss: 245201.2500 - val_recall_7: 0.4628 - val_accuracy: 0.4628 - val_precision_7: 0.4628\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 155097.4688 - recall_7: 0.6279 - accuracy: 0.6279 - precision_7: 0.6279 - val_loss: 197258.4531 - val_recall_7: 0.4959 - val_accuracy: 0.4959 - val_precision_7: 0.4959\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 130204.8047 - recall_7: 0.6445 - accuracy: 0.6445 - precision_7: 0.6445 - val_loss: 197538.5469 - val_recall_7: 0.4876 - val_accuracy: 0.4876 - val_precision_7: 0.4876\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 127431.6172 - recall_7: 0.6320 - accuracy: 0.6320 - precision_7: 0.6320 - val_loss: 208605.0938 - val_recall_7: 0.4959 - val_accuracy: 0.4959 - val_precision_7: 0.4959\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 120506.5781 - recall_7: 0.6445 - accuracy: 0.6445 - precision_7: 0.6445 - val_loss: 205585.3125 - val_recall_7: 0.4463 - val_accuracy: 0.4463 - val_precision_7: 0.4463\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 122164.1250 - recall_7: 0.6279 - accuracy: 0.6279 - precision_7: 0.6279 - val_loss: 190900.5312 - val_recall_7: 0.4793 - val_accuracy: 0.4793 - val_precision_7: 0.4793\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 117002.4609 - recall_7: 0.6320 - accuracy: 0.6320 - precision_7: 0.6320 - val_loss: 172504.7656 - val_recall_7: 0.4793 - val_accuracy: 0.4793 - val_precision_7: 0.4793\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 95228.4375 - recall_7: 0.6632 - accuracy: 0.6632 - precision_7: 0.6632 - val_loss: 210289.5625 - val_recall_7: 0.4380 - val_accuracy: 0.4380 - val_precision_7: 0.4380\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 123652.7891 - recall_7: 0.6383 - accuracy: 0.6383 - precision_7: 0.6383 - val_loss: 173093.2500 - val_recall_7: 0.5041 - val_accuracy: 0.5041 - val_precision_7: 0.5041\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 89944.6641 - recall_7: 0.6861 - accuracy: 0.6861 - precision_7: 0.6861 - val_loss: 192846.8438 - val_recall_7: 0.4628 - val_accuracy: 0.4628 - val_precision_7: 0.4628\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 116776.3438 - recall_7: 0.6466 - accuracy: 0.6466 - precision_7: 0.6466 - val_loss: 159393.9844 - val_recall_7: 0.4793 - val_accuracy: 0.4793 - val_precision_7: 0.4793\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 76105.8906 - recall_7: 0.7006 - accuracy: 0.7006 - precision_7: 0.7006 - val_loss: 188611.1094 - val_recall_7: 0.4876 - val_accuracy: 0.4876 - val_precision_7: 0.4876\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 97646.9453 - recall_7: 0.6486 - accuracy: 0.6486 - precision_7: 0.6486 - val_loss: 159666.0938 - val_recall_7: 0.4876 - val_accuracy: 0.4876 - val_precision_7: 0.4876\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 74651.2188 - recall_7: 0.6985 - accuracy: 0.6985 - precision_7: 0.6985 - val_loss: 165081.0156 - val_recall_7: 0.4545 - val_accuracy: 0.4545 - val_precision_7: 0.4545\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 79563.5078 - recall_7: 0.6653 - accuracy: 0.6653 - precision_7: 0.6653 - val_loss: 172960.1094 - val_recall_7: 0.4463 - val_accuracy: 0.4463 - val_precision_7: 0.4463\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 82275.8828 - recall_7: 0.6736 - accuracy: 0.6736 - precision_7: 0.6736 - val_loss: 154391.6250 - val_recall_7: 0.5041 - val_accuracy: 0.5041 - val_precision_7: 0.5041\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 64785.1914 - recall_7: 0.7027 - accuracy: 0.7027 - precision_7: 0.7027 - val_loss: 168015.1250 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 79914.3750 - recall_7: 0.6736 - accuracy: 0.6736 - precision_7: 0.6736 - val_loss: 163148.1562 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 70532.9844 - recall_7: 0.6881 - accuracy: 0.6881 - precision_7: 0.6881 - val_loss: 153528.2031 - val_recall_7: 0.5207 - val_accuracy: 0.5207 - val_precision_7: 0.5207\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 60423.4102 - recall_7: 0.7131 - accuracy: 0.7131 - precision_7: 0.7131 - val_loss: 167378.6719 - val_recall_7: 0.4628 - val_accuracy: 0.4628 - val_precision_7: 0.4628\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 75771.7188 - recall_7: 0.6757 - accuracy: 0.6757 - precision_7: 0.6757 - val_loss: 167906.2812 - val_recall_7: 0.4215 - val_accuracy: 0.4215 - val_precision_7: 0.4215\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 73786.5000 - recall_7: 0.6944 - accuracy: 0.6944 - precision_7: 0.6944 - val_loss: 164911.8281 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 70277.4922 - recall_7: 0.7048 - accuracy: 0.7048 - precision_7: 0.7048 - val_loss: 204821.9062 - val_recall_7: 0.4793 - val_accuracy: 0.4793 - val_precision_7: 0.4793\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 116282.0312 - recall_7: 0.7152 - accuracy: 0.7152 - precision_7: 0.7152 - val_loss: 202028.3438 - val_recall_7: 0.4959 - val_accuracy: 0.4959 - val_precision_7: 0.4959\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 101819.7188 - recall_7: 0.7173 - accuracy: 0.7173 - precision_7: 0.7173 - val_loss: 171742.9844 - val_recall_7: 0.4380 - val_accuracy: 0.4380 - val_precision_7: 0.4380\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 78180.2578 - recall_7: 0.7173 - accuracy: 0.7173 - precision_7: 0.7173 - val_loss: 159642.4531 - val_recall_7: 0.4545 - val_accuracy: 0.4545 - val_precision_7: 0.4545\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 63817.6562 - recall_7: 0.6902 - accuracy: 0.6902 - precision_7: 0.6902 - val_loss: 155421.6719 - val_recall_7: 0.4793 - val_accuracy: 0.4793 - val_precision_7: 0.4793\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 58414.8672 - recall_7: 0.7048 - accuracy: 0.7048 - precision_7: 0.7048 - val_loss: 167638.5156 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 74615.3438 - recall_7: 0.7173 - accuracy: 0.7173 - precision_7: 0.7173 - val_loss: 206708.1719 - val_recall_7: 0.4463 - val_accuracy: 0.4463 - val_precision_7: 0.4463\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 108034.8203 - recall_7: 0.6923 - accuracy: 0.6923 - precision_7: 0.6923 - val_loss: 176385.0938 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 86034.5625 - recall_7: 0.7256 - accuracy: 0.7256 - precision_7: 0.7256 - val_loss: 167737.2031 - val_recall_7: 0.4793 - val_accuracy: 0.4793 - val_precision_7: 0.4793\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 69044.3672 - recall_7: 0.7193 - accuracy: 0.7193 - precision_7: 0.7193 - val_loss: 145303.7188 - val_recall_7: 0.4628 - val_accuracy: 0.4628 - val_precision_7: 0.4628\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 52100.8281 - recall_7: 0.7360 - accuracy: 0.7360 - precision_7: 0.7360 - val_loss: 142137.3281 - val_recall_7: 0.4876 - val_accuracy: 0.4876 - val_precision_7: 0.4876\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 46451.4648 - recall_7: 0.7380 - accuracy: 0.7380 - precision_7: 0.7380 - val_loss: 139466.0312 - val_recall_7: 0.4628 - val_accuracy: 0.4628 - val_precision_7: 0.4628\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 44713.2344 - recall_7: 0.7277 - accuracy: 0.7277 - precision_7: 0.7277 - val_loss: 145712.5156 - val_recall_7: 0.4793 - val_accuracy: 0.4793 - val_precision_7: 0.4793\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 48437.2500 - recall_7: 0.7318 - accuracy: 0.7318 - precision_7: 0.7318 - val_loss: 160041.3125 - val_recall_7: 0.4959 - val_accuracy: 0.4959 - val_precision_7: 0.4959\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 69705.6562 - recall_7: 0.7193 - accuracy: 0.7193 - precision_7: 0.7193 - val_loss: 219062.2344 - val_recall_7: 0.4959 - val_accuracy: 0.4959 - val_precision_7: 0.4959\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 116772.8125 - recall_7: 0.7339 - accuracy: 0.7339 - precision_7: 0.7339 - val_loss: 193662.8906 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 107574.1719 - recall_7: 0.7380 - accuracy: 0.7380 - precision_7: 0.7380 - val_loss: 171415.1562 - val_recall_7: 0.4876 - val_accuracy: 0.4876 - val_precision_7: 0.4876\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 71361.2422 - recall_7: 0.7360 - accuracy: 0.7360 - precision_7: 0.7360 - val_loss: 158638.8594 - val_recall_7: 0.4959 - val_accuracy: 0.4959 - val_precision_7: 0.4959\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 65878.5938 - recall_7: 0.7339 - accuracy: 0.7339 - precision_7: 0.7339 - val_loss: 186334.0312 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 85652.0547 - recall_7: 0.7318 - accuracy: 0.7318 - precision_7: 0.7318 - val_loss: 194517.9531 - val_recall_7: 0.4959 - val_accuracy: 0.4959 - val_precision_7: 0.4959\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 105781.2578 - recall_7: 0.7609 - accuracy: 0.7609 - precision_7: 0.7609 - val_loss: 204386.4688 - val_recall_7: 0.4628 - val_accuracy: 0.4628 - val_precision_7: 0.4628\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 98377.2734 - recall_7: 0.7318 - accuracy: 0.7318 - precision_7: 0.7318 - val_loss: 163495.7500 - val_recall_7: 0.4380 - val_accuracy: 0.4380 - val_precision_7: 0.4380\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 69399.0156 - recall_7: 0.7339 - accuracy: 0.7339 - precision_7: 0.7339 - val_loss: 160236.9062 - val_recall_7: 0.4793 - val_accuracy: 0.4793 - val_precision_7: 0.4793\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 55864.6680 - recall_7: 0.7110 - accuracy: 0.7110 - precision_7: 0.7110 - val_loss: 148427.2969 - val_recall_7: 0.4215 - val_accuracy: 0.4215 - val_precision_7: 0.4215\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 51557.9727 - recall_7: 0.7256 - accuracy: 0.7256 - precision_7: 0.7256 - val_loss: 145112.1719 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 43972.5898 - recall_7: 0.7277 - accuracy: 0.7277 - precision_7: 0.7277 - val_loss: 141976.0156 - val_recall_7: 0.4628 - val_accuracy: 0.4628 - val_precision_7: 0.4628\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 42900.8164 - recall_7: 0.7484 - accuracy: 0.7484 - precision_7: 0.7484 - val_loss: 158769.2969 - val_recall_7: 0.4793 - val_accuracy: 0.4793 - val_precision_7: 0.4793\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 63464.2812 - recall_7: 0.7588 - accuracy: 0.7588 - precision_7: 0.7588 - val_loss: 233772.2344 - val_recall_7: 0.4876 - val_accuracy: 0.4876 - val_precision_7: 0.4876\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 124769.7969 - recall_7: 0.7651 - accuracy: 0.7651 - precision_7: 0.7651 - val_loss: 187159.7188 - val_recall_7: 0.4545 - val_accuracy: 0.4545 - val_precision_7: 0.4545\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 93857.5312 - recall_7: 0.7401 - accuracy: 0.7401 - precision_7: 0.7401 - val_loss: 141541.4062 - val_recall_7: 0.4711 - val_accuracy: 0.4711 - val_precision_7: 0.4711\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 40414.6836 - recall_7: 0.7630 - accuracy: 0.7630 - precision_7: 0.7630 - val_loss: 151365.9375 - val_recall_7: 0.4959 - val_accuracy: 0.4959 - val_precision_7: 0.4959\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 49195.0430 - recall_7: 0.7131 - accuracy: 0.7131 - precision_7: 0.7131Restoring model weights from the end of the best epoch: 100.\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 49195.0430 - recall_7: 0.7131 - accuracy: 0.7131 - precision_7: 0.7131 - val_loss: 174073.0156 - val_recall_7: 0.4298 - val_accuracy: 0.4298 - val_precision_7: 0.4298\n",
      "Epoch 120: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "epochs = 1000\n",
    "patience = 20\n",
    "l2_value = 0.006\n",
    "input_shape = (n_timesteps, n_features)\n",
    "\n",
    "model = build_model(input_shape, l2_value)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer=Adam(),\n",
    "            metrics=[Recall(), 'accuracy', Precision()])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        patience=patience,\n",
    "                        verbose=2,\n",
    "                        mode='min',\n",
    "                        restore_best_weights=True,\n",
    "                    )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.5114503816793893\n",
      "Confusion Matrix:\n",
      " [[24 28]\n",
      " [36 43]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.46      0.43        52\n",
      "           1       0.61      0.54      0.57        79\n",
      "\n",
      "    accuracy                           0.51       131\n",
      "   macro avg       0.50      0.50      0.50       131\n",
      "weighted avg       0.52      0.51      0.52       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.array([1 if entry[0] < entry[1] else 0 for entry in y_pred])\n",
    "y_true = y_test_class\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "report = classification_report(y_true, y_pred_classes)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
