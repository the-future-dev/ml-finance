{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 15:49:26.314296: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-06 15:49:26.345204: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-06 15:49:26.345234: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-06 15:49:26.345250: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-06 15:49:26.349809: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-06 15:49:26.936595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Conv1D, Flatten, MaxPooling1D, Dense, Reshape, Dropout, LeakyReLU, MultiHeadAttention, TimeDistributed, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from shared import read_dataset, plot_results, evaluate_price_predictions, mean_abs_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Trading Days: (2541, 23)\n",
      "['Close', 'Open', 'High', 'Low', 'Volume', 'ma7', 'ma7_diff', 'ma21', 'ma21_diff', '26ema', '26ema_diff', '12ema', '12ema_diff', 'MACD', 'upper_band', 'lower_band', 'momentum', 'fourier_short', 'fourier_medium', 'fourier_long', 'Volatility_21', 'Close Diff', 'Open Diff']\n"
     ]
    }
   ],
   "source": [
    "start = \"2013-10-01\"\n",
    "end = \"2023-10-01\"\n",
    "target_column_name = 'Close'\n",
    "path = '../models/lstm_model/predictor_adj_close.h5'\n",
    "\n",
    "data = read_dataset('../data/DAL.MI_ta.csv', start, end)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "cols = [target_column_name] + [ col for col in data if col != target_column_name]\n",
    "target_column = list(data.columns).index(target_column_name)\n",
    "data = data[cols]\n",
    "\n",
    "print(f\"#Trading Days: {data.shape}\")\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature array and target array to train the model.\n",
    "data_array = np.array(data.values)\n",
    "target_array = np.array(data[target_column_name].values).reshape(-1, 1)\n",
    "\n",
    "# Normalize the data\n",
    "scaler_data = MinMaxScaler()\n",
    "scaler_data.fit(data_array)\n",
    "data_array = scaler_data.transform(data_array)\n",
    "\n",
    "scaler_target = MinMaxScaler()\n",
    "scaler_target.fit(target_array)\n",
    "target_array = scaler_target.transform(target_array)\n",
    "\n",
    "# Split the data\n",
    "train_size = int(len(data_array) * 0.70)\n",
    "\n",
    "def create_sequences(data, target, seq_length):\n",
    "    sequence_data = []\n",
    "    sequence_target = []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        sequence_data.append(data[i-seq_length:i])\n",
    "        sequence_target.append(target[i])\n",
    "    return np.array(sequence_data), np.array(sequence_target)\n",
    "\n",
    "SEQUENCE_LENGTH = 100\n",
    "data_sequences, target_sequences = create_sequences(data_array, target_array, SEQUENCE_LENGTH)\n",
    "\n",
    "train_data, test_data = data_sequences[:train_size], data_sequences[train_size:]\n",
    "train_target, test_target = target_sequences[:train_size], target_sequences[train_size:]\n",
    "\n",
    "print(f'train_data: {train_data.shape} triat_target: {train_target.shape}')\n",
    "print(f'test_data: {test_data.shape} test_target: {test_target.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "We will use three different types of Deep Neural Networks:\n",
    " - LSTM\n",
    " - CNN\n",
    " - Dense\n",
    "\n",
    "At first we will train independently the CNN through a VAE and we will use the encoder for feature extrapolation.\n",
    "Then we'll combine the three branches and train the model.\n",
    "\n",
    "### VAE (Variational Auto Encoder)\n",
    "As a feature extractor for our main neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Sampling(keras.layers.Layer):\n",
    "#     def call(self, inputs):\n",
    "#         z_mean, z_log_var = inputs\n",
    "#         batch = tf.shape(z_mean)[0]\n",
    "#         dim = tf.shape(z_mean)[1]\n",
    "#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# def create_encoder(sequence_length, n_features, latent_dim):\n",
    "#     inputs = Input(shape=(sequence_length, n_features))\n",
    "#     x = Conv1D(filters=32, kernel_size=3, activation='relu')(inputs)\n",
    "#     x = Conv1D(filters=64, kernel_size=3, activation=LeakyReLU(alpha=0.1))(x)\n",
    "#     x = MaxPooling1D(pool_size=1)(x)\n",
    "#     x = Conv1D(filters=128, kernel_size=2, activation=LeakyReLU(alpha=0.1))(x)\n",
    "#     x = MaxPooling1D(pool_size=1)(x)\n",
    "#     x = Conv1D(filters=256, kernel_size=2, activation=LeakyReLU(alpha=0.1))(x)\n",
    "#     x = MaxPooling1D(pool_size=1)(x)\n",
    "#     x = Conv1D(filters=256, kernel_size=2, activation=LeakyReLU(alpha=0.1))(x)\n",
    "#     x = MaxPooling1D(pool_size=1)(x)\n",
    "#     x = Conv1D(filters=256, kernel_size=2, activation=LeakyReLU(alpha=0.1))(x)\n",
    "#     x = MaxPooling1D(pool_size=1)(x)\n",
    "#     x = Flatten()(x)\n",
    "#     z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "#     z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "#     encoder = Model(inputs, [z_mean, z_log_var], name='encoder')\n",
    "#     return encoder\n",
    "\n",
    "# def create_decoder(sequence_length, n_features, latent_dim):\n",
    "#     latent_inputs = Input(shape=(latent_dim,))\n",
    "#     x = Dense(sequence_length * n_features, activation='relu')(latent_inputs)\n",
    "#     x = Reshape((sequence_length, n_features))(x)\n",
    "#     # x = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "#     outputs = Conv1D(filters=n_features, kernel_size=3, activation='sigmoid', padding='same')(x)\n",
    "#     decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "#     return decoder\n",
    "\n",
    "# class VAE(keras.Model):\n",
    "#     def __init__(self, encoder, decoder, beta=1.0, **kwargs):\n",
    "#         super(VAE, self).__init__(**kwargs)\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "#         self.sampling = Sampling()\n",
    "#         self.beta = beta\n",
    "\n",
    "#     def train_step(self, data):\n",
    "#         if isinstance(data, tuple):\n",
    "#             data = data[0]\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             z_mean, z_log_var = self.encoder(data)\n",
    "#             z = self.sampling((z_mean, z_log_var))\n",
    "#             reconstruction = self.decoder(z)\n",
    "#             reconstruction_loss = tf.reduce_mean(\n",
    "#                 tf.reduce_sum(\n",
    "#                     keras.losses.mean_squared_error(data, reconstruction), axis=1\n",
    "#                 )\n",
    "#             )\n",
    "#             kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "#             kl_loss = self.beta * tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))  # Weighted KL loss\n",
    "#             total_loss = reconstruction_loss + kl_loss\n",
    "#         grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "#         self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "#         return {\n",
    "#             \"total_loss\": total_loss,\n",
    "#             \"reconstruction_loss\": reconstruction_loss,\n",
    "#             \"kl_loss\": kl_loss,\n",
    "#         }\n",
    "    \n",
    "#     def test_step(self, data):\n",
    "#         if isinstance(data, tuple):\n",
    "#             data = data[0]\n",
    "#         z_mean, z_log_var = self.encoder(data)\n",
    "#         z = self.sampling((z_mean, z_log_var))\n",
    "#         reconstruction = self.decoder(z)\n",
    "#         reconstruction_loss = tf.reduce_mean(\n",
    "#             tf.reduce_sum(\n",
    "#                 keras.losses.mean_squared_error(data, reconstruction), axis=1\n",
    "#             )\n",
    "#         )\n",
    "#         kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "#         kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "#         total_loss = reconstruction_loss + kl_loss\n",
    "#         return {\n",
    "#             \"loss\": total_loss,\n",
    "#             \"reconstruction_loss\": reconstruction_loss,\n",
    "#             \"kl_loss\": kl_loss,\n",
    "#         }\n",
    "\n",
    "# def build_autoencoder(encoder, decoder, learning_rate, beta):\n",
    "#     vae = VAE(encoder, decoder, beta=beta)\n",
    "#     vae.compile(optimizer=Adam(learning_rate=learning_rate))\n",
    "#     return vae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_dim = 3\n",
    "# epochs = 1000\n",
    "# batch_size = 1024\n",
    "# patience = 20\n",
    "# encoder = create_encoder(sequence_length=train_data.shape[1], n_features=train_data.shape[2], latent_dim=latent_dim)\n",
    "# decoder = create_decoder(sequence_length=train_data.shape[1], n_features=train_data.shape[2], latent_dim=latent_dim)\n",
    "\n",
    "# vae = build_autoencoder(encoder, decoder, learning_rate=0.0003, beta=0.3)\n",
    "\n",
    "# print(\"\\n\")\n",
    "# test_loss = vae.evaluate(test_data, test_data)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# early_stopping = keras.callbacks.EarlyStopping(\n",
    "#     monitor='kl_loss',\n",
    "#     patience=patience,\n",
    "#     verbose=2,\n",
    "#     mode='min',\n",
    "#     restore_best_weights=True,\n",
    "# )\n",
    "\n",
    "# vae.fit(train_data, train_data, epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "# print(\"\\n\")\n",
    "# test_loss = vae.evaluate(test_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss of the test_data on the CNN before training:<br>\n",
    "> loss: 9.6933 - reconstruction_loss: 1.1233 - kl_loss: 8.5700\n",
    "\n",
    "Loss of the test_data on the CNN post training: <br>\n",
    "> loss: 0.4672 - reconstruction_loss: 0.4509 - kl_loss: 0.0163\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "I observed lower decrease in the reconstruction loss using the difference prices in comparison with the timeseries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Model Function Definition\n",
    "Three branches with independent input all converge to a dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_parallel_model(input_shape, encoder=0, l2_value=0.01):\n",
    "    # LSTM Branch\n",
    "    lstm_input = keras.layers.Input(shape=input_shape)\n",
    "    lstm_branch = keras.layers.LSTM(90, kernel_regularizer=l2(l2_value), recurrent_regularizer=l2(l2_value), bias_regularizer=l2(l2_value))(lstm_input)\n",
    "\n",
    "    # Dense Branch\n",
    "    # dense_input = keras.layers.Input(shape=input_shape)\n",
    "    # flattened = keras.layers.Flatten()(dense_input)\n",
    "    # dense_branch = keras.layers.Dense(32, activation='relu', kernel_regularizer=l2(l2_value))(flattened)\n",
    "    # dense_branch = keras.layers.Dense(5)(dense_branch)\n",
    "\n",
    "    # VAE Branch\n",
    "    # vae_input = keras.layers.Input(shape=input_shape)\n",
    "    # z_mean, z_log_var = encoder(vae_input) # Use only z_mean for subsequent layers\n",
    "    # encoded_output = keras.layers.Flatten()(z_mean)\n",
    "\n",
    "    # Combining the branches\n",
    "    combined = keras.layers.concatenate([\n",
    "        lstm_branch,\n",
    "        # dense_branch,\n",
    "        # encoded_output,\n",
    "        # cnn_output\n",
    "    ])\n",
    "\n",
    "    # Additional layers after combining\n",
    "    combined_dense = keras.layers.Dense(units=1, kernel_regularizer=l2(l2_value))(combined)\n",
    "    output = keras.layers.Dense(1)(combined_dense)\n",
    "\n",
    "    model = keras.models.Model(inputs=[\n",
    "        lstm_input,\n",
    "        # dense_input,\n",
    "        # vae_input,\n",
    "        # cnn_input,\n",
    "        ], outputs=output)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_data, train_target, epochs=30, batch_size=256, patience=20):\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,\n",
    "        verbose=2,\n",
    "        mode='min',\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    # live_plot = LivePlotCallback()\n",
    "\n",
    "    model.fit(\n",
    "        train_data,  # Assuming both branches use the same training data\n",
    "        train_target,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=2,\n",
    "        validation_split=0.3,\n",
    "        callbacks=[early_stopping]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_attention_lstm(seq_length, num_features):\n",
    "    \n",
    "    # Attention Model\n",
    "    input_layer = Input(shape=(seq_length, num_features))\n",
    "    # flatten = TimeDistributed(Flatten()) (input_layer)\n",
    "    attention_output = MultiHeadAttention(num_heads=10, key_dim=64)(input_layer, input_layer, input_layer)\n",
    "    dense_attention = Dense(1) (attention_output)\n",
    "    \n",
    "    # LSTM Model\n",
    "    lstm_output = LSTM(64, return_sequences=True)(input_layer)\n",
    "    dense_lstm = Dense(1)(lstm_output)\n",
    "    \n",
    "    # Merge the outputs\n",
    "    merged = keras.layers.concatenate([dense_attention, dense_lstm])\n",
    "    \n",
    "    output = keras.layers.Dense(3)(merged)\n",
    "\n",
    "    return Model(input_layer, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 21:56:58.368561: W tensorflow/core/framework/op_kernel.cc:1827] INVALID_ARGUMENT: required broadcastable shapes\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node mean_squared_error/SquaredDifference defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1046, in launch_instance\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n\n  File \"/tmp/ipykernel_30537/3558612636.py\", line 12, in <module>\n\n  File \"/tmp/ipykernel_30537/3601051681.py\", line 50, in train_model\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/losses.py\", line 1706, in mean_squared_error\n\nrequired broadcastable shapes\n\t [[{{node mean_squared_error/SquaredDifference}}]] [Op:__inference_train_function_20319]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# model = load_model(\"../models/3branches.h5\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_model(model, train_data, train_target, epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size, patience\u001b[39m=\u001b[39;49mpatience)\n",
      "\u001b[1;32m/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb Cell 14\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m early_stopping \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     patience\u001b[39m=\u001b[39mpatience,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# live_plot = LivePlotCallback()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     train_data,  \u001b[39m# Assuming both branches use the same training data\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     train_target,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[early_stopping]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/main.ipynb#X15sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node mean_squared_error/SquaredDifference defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1046, in launch_instance\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n\n  File \"/tmp/ipykernel_30537/3558612636.py\", line 12, in <module>\n\n  File \"/tmp/ipykernel_30537/3601051681.py\", line 50, in train_model\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/losses.py\", line 1706, in mean_squared_error\n\nrequired broadcastable shapes\n\t [[{{node mean_squared_error/SquaredDifference}}]] [Op:__inference_train_function_20319]"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "epochs = 3000\n",
    "patience = 20\n",
    "l2_value = 0.03\n",
    "\n",
    "## GENERATE MODEL ##\n",
    "model = build_attention_lstm(train_data.shape[1], train_data.shape[2])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "model.fit(train_data, train_target, epochs=epochs, batch_size=batch_size)\n",
    "# model = load_model(\"../models/3branches.h5\")\n",
    "# train_model(model, train_data, train_target, epochs=epochs, batch_size=batch_size, patience=patience)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_predict = (train_data)\n",
    "actual_prediction = train_target\n",
    "\n",
    "model.evaluate(data_to_predict, actual_prediction)\n",
    "\n",
    "# TESTing: DENORMALIZ300E TARGET AND PREDICTIONS ##\n",
    "price_predicted_array = scaler_target.inverse_transform(model.predict(data_to_predict)) #[1:]\n",
    "price_actual_array = scaler_target.inverse_transform(actual_prediction).flatten() #[:-1]\n",
    "\n",
    "## Evaluation\n",
    "evaluate_price_predictions(price_predicted_array.flatten(), price_actual_array)\n",
    "\n",
    "## PLOTting #\n",
    "plot_results(price_actual_array, price_predicted_array, target_column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training:\n",
    "1st with 2010-01-01 - 2023-01-01 data\n",
    "-> then save\n",
    "\n",
    "2nd use 2023-01-01  - 2023-11-18 data\n",
    "-> override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../models/3branches/low.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
