{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-04 19:14:10.224666: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-04 19:14:10.510179: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-04 19:14:10.510210: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-04 19:14:10.511556: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-04 19:14:10.621498: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-04 19:14:11.549981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from shared import read_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"2010-01-01\"\n",
    "end = \"2021-01-01\"\n",
    "\n",
    "commodities = read_dataset('../data/commodities.csv', start, end)\n",
    "company = read_dataset('../data/company.csv', start, end)\n",
    "company = company.drop('CS', axis=1)\n",
    "fixed_income = read_dataset('../data/fixed_income.csv', start, end)\n",
    "forex = read_dataset('../data/forex.csv', start, end)\n",
    "GS = read_dataset('../data/GS.csv', start, end)\n",
    "us_macro = read_dataset('../data/us_macro.csv', start, end)\n",
    "vix = read_dataset('../data/vix.csv', start, end)\n",
    "\n",
    "GS['Close_Diff'] = GS['Close'].diff()\n",
    "GS.dropna(inplace=True)\n",
    "\n",
    "data = pd.concat([\n",
    "    # company,\n",
    "    # commodities,\n",
    "    # fixed_income,\n",
    "    # forex,\n",
    "    # us_macro,\n",
    "    GS,\n",
    "    # vix\n",
    "    ], axis=1)\n",
    "\n",
    "target_column_name = 'Adj Close'\n",
    "\n",
    "# Reorder columns to have 'target_column' as the first column:\n",
    "cols = [target_column_name] + [ col for col in data if col != target_column_name]\n",
    "target_column = list(data.columns).index(target_column_name)\n",
    "data = data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction_power(target_series, predictor_series):\n",
    "    # Ensure the indices are datetime objects and the series are aligned by date\n",
    "    target_series.index = pd.to_datetime(target_series.index)\n",
    "    predictor_series.index = pd.to_datetime(predictor_series.index)\n",
    "    merged_data = pd.merge(target_series, predictor_series, left_index=True, right_index=True)\n",
    "    \n",
    "    # Drop rows with missing or infinite values\n",
    "    merged_data = merged_data.dropna().replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    # Perform linear regression\n",
    "    X = merged_data.iloc[:, 1]  # Predictor Series\n",
    "    y = merged_data.iloc[:, 0]  # Target Series\n",
    "    X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
    "\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = results.predict(X)\n",
    "    r_squared = r2_score(y, predictions)\n",
    "    \n",
    "    return r_squared, results.summary()\n",
    "\n",
    "def get_top_five(dataframe, target_series):\n",
    "    results_list = []\n",
    "    for column in dataframe.columns:\n",
    "        predictor_series = dataframe[column]\n",
    "        r_squared, summary = evaluate_prediction_power(target_series, predictor_series)\n",
    "        results_list.append((column, r_squared))\n",
    "    sorted_results = sorted(results_list, key=lambda x: x[1], reverse=True)\n",
    "    top_5_columns = [item[0] for item in sorted_results[:5]]\n",
    "    return dataframe[top_5_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Generative Adversarial Network (GAN)\n",
    "- LSTM Generator\n",
    "- CNN Discriminator\n",
    "\n",
    "\n",
    "Unconventional GAN.\n",
    "\n",
    "> Feed the LSTM Generator with the real data and make it output an y: a four days price forecast.\n",
    "> Than use the CNN discriminator to see if the forecast is feasible or not.\n",
    "\n",
    "The discriminator is trained with the real data and its forecast and has to classify binary if the forecast is real or generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3559/3280862857.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data.fillna(method='bfill', inplace=True)\n",
      "/tmp/ipykernel_3559/3280862857.py:4: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# # Data refactoring\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data.fillna(method='bfill', inplace=True)\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "data_array = np.array(data.values)\n",
    "target_array = np.array(data[target_column_name].values).reshape(-1, 1)\n",
    "\n",
    "# # Data scaling\n",
    "# data_array = data[:-1].values\n",
    "# target_array = data[target_column_name].shift(-1).values[:-1]\n",
    "\n",
    "# data_array = np.array([i for i in range(2000)]).reshape(-1, 1)\n",
    "# target_array = np.array([a_ + 1 for a_ in data_array]).reshape(-1, 1)\n",
    "\n",
    "scaler_data = MinMaxScaler()\n",
    "scaler_data.fit(data_array)\n",
    "data_array = scaler_data.transform(data_array)\n",
    "\n",
    "scaler_target = MinMaxScaler()\n",
    "scaler_target.fit(target_array)\n",
    "target_array = scaler_target.transform(target_array)\n",
    "\n",
    "# Data splitting\n",
    "train_size = int(len(data_array) * 0.70)\n",
    "evaluation_size = int(len(data_array) * 0.10)\n",
    "\n",
    "\n",
    "def create_sequences(data, target, seq_length):\n",
    "    sequence_data = []\n",
    "    sequence_target = []\n",
    "    for i in range(seq_length, len(data)+1):\n",
    "        sequence_data.append(data[i-seq_length:i])\n",
    "        sequence_target.append(target[i-1])\n",
    "    return np.array(sequence_data), np.array(sequence_target)\n",
    "\n",
    "SEQUENCE_LENGTH = 17\n",
    "data_sequences, target_sequences = create_sequences(data_array, target_array, SEQUENCE_LENGTH)\n",
    "\n",
    "shuffle_idxs = np.random.permutation(len(data_sequences))\n",
    "revert_idxs = np.argsort(shuffle_idxs)\n",
    "\n",
    "data_sequences = data_sequences[shuffle_idxs]\n",
    "target_sequences = target_sequences[shuffle_idxs]\n",
    "\n",
    "train_data, eval_data, test_data = data_sequences[:train_size], data_sequences[train_size:train_size+evaluation_size], data_sequences[train_size+evaluation_size:]\n",
    "train_target, eval_target, test_target = target_sequences[:train_size], target_sequences[train_size:train_size+evaluation_size], target_sequences[train_size+evaluation_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_day_trading = []\n",
    "# for i in range (train_data.shape[0]):\n",
    "#     w = scaler_data.inverse_transform(train_data[i])\n",
    "#     last_day_trading.append(w[-1])\n",
    "\n",
    "# denormalized_target_array = scaler_target.inverse_transform(train_target)\n",
    "\n",
    "# print(last_day_trading)\n",
    "# print(denormalized_target_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_d = 0.005  # discriminator learning rate\n",
    "lr_g = 0.001   # generator learning rate\n",
    "NOISE_DIM = 100\n",
    "\n",
    "def build_generator():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.LSTM(units=256, return_sequences=True, input_shape=(SEQUENCE_LENGTH, data.shape[1])))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.LSTM(units=128, return_sequences=True, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.LSTM(units=64, return_sequences=True, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.LSTM(units=32, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.LSTM(units=16, return_sequences=False))\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "def train_just_generator(model, train_features, train_target, eval_features, eval_target, epochs=1000, batch_size=10):\n",
    "    #early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(train_features, train_target)\n",
    "    # model.fit(train_features, train_target,\n",
    "    #           validation_data=(eval_features, eval_target),\n",
    "    #           epochs=epochs,\n",
    "    #           batch_size=batch_size,\n",
    "    #           verbose=2,\n",
    "    #           #callbacks=[early_stopping]\n",
    "    #           )\n",
    "\n",
    "def build_discriminator():\n",
    "    # Input for the sequence\n",
    "    sequence_input = keras.Input(shape=(SEQUENCE_LENGTH, data.shape[1]))\n",
    "    x = layers.LSTM(64, return_sequences=False)(sequence_input)\n",
    "    \n",
    "    # Input for the forecasted value\n",
    "    forecast_input = keras.Input(shape=(1,))\n",
    "    \n",
    "    # Concatenate the sequence features and the forecasted value\n",
    "    merged = keras.layers.Concatenate()([x, forecast_input])\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu')(merged)\n",
    "    validity = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=[sequence_input, forecast_input], outputs=validity)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr_d), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    noise = keras.Input(shape=(SEQUENCE_LENGTH, data.shape[1]))\n",
    "    generated_sequence = generator(noise)\n",
    "    validity = discriminator([noise, generated_sequence])\n",
    "    combined = keras.Model(noise, validity)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr_g))\n",
    "    return combined\n",
    "\n",
    "def train_gan(data, generator, discriminator, combined, epochs=1000, batch_size=128):\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "        real_sequences_input = data[idx, :, :]\n",
    "        real_forecast = data[idx, -1, target_column].reshape(batch_size, 1)  # One-day forecast\n",
    "\n",
    "        generated_forecast = generator.predict(real_sequences_input)\n",
    "\n",
    "        # Train discriminator\n",
    "        d_loss_real = discriminator.train_on_batch([real_sequences_input, real_forecast], np.ones((batch_size, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch([real_sequences_input, generated_forecast], np.zeros((batch_size, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train generator\n",
    "        g_loss = combined.train_on_batch(real_sequences_input, np.ones((batch_size, 1)))\n",
    "\n",
    "        d_losses.append(d_loss[0])\n",
    "        g_losses.append(g_loss)\n",
    "        # print(f\"{epoch}/{epochs} [D loss: {d_loss[0]:.4f}] [G loss: {g_loss[0]:.4f}]\")\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(d_losses, label=\"Discriminator Loss\")\n",
    "    plt.plot(g_losses, label=\"Generator Loss\")\n",
    "    plt.title(\"GAN Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "generator = load_model('../models/generator.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 2, 21), found shape=(None, 17, 21)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/model_creation.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/model_creation.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m generator \u001b[39m=\u001b[39m build_generator()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/model_creation.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_just_generator(generator, train_data, train_target, eval_data, eval_target, epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/model_creation.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# generator.save('../models/generator.h5')\u001b[39;00m\n",
      "\u001b[1;32m/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/model_creation.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/model_creation.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_just_generator\u001b[39m(model, train_features, train_target, eval_features, eval_target, epochs\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/model_creation.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m#early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/model_creation.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/andrea/Desktop/ml-finance/stock-predictor-ai/code/model_creation.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(train_features, train_target)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filem9itoe6h.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/andrea/.local/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 2, 21), found shape=(None, 17, 21)\n"
     ]
    }
   ],
   "source": [
    "generator = build_generator()\n",
    "train_just_generator(generator, train_data, train_target, eval_data, eval_target, epochs=1000, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "combined = build_gan(generator, discriminator)\n",
    "\n",
    "train_gan(train_data, generator, discriminator, combined, epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = generator.evaluate(test_data, test_target)\n",
    "\n",
    "recomposed_target = np.concatenate([train_target, eval_target, test_target])\n",
    "recomposed_target = recomposed_target[revert_idxs]\n",
    "\n",
    "predicted_target = np.concatenate([train_target, eval_target, generator.predict(test_data)])\n",
    "predicted_target = predicted_target[revert_idxs]\n",
    "\n",
    "# Reverse the scaling transformation to get the original price values\n",
    "price_predicted_array = scaler_target.inverse_transform(predicted_target)\n",
    "price_actual_array = scaler_target.inverse_transform(recomposed_target.reshape(-1, 1)).flatten()\n",
    "\n",
    "# print(f\"Testa data evaluation: loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the first 200 actual vs predicted prices\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.plot(price_actual_array, label=\"Real\", color='blue')\n",
    "plt.scatter(range(len(price_predicted_array)), price_predicted_array, label=\"Predicted\", color='red', marker='o')\n",
    "\n",
    "# Title and labels\n",
    "#plt.title(f\"Actual vs Predicted {target_column_name}\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "#plt.ylabel(f\"{target_column_name}\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('../models/light/price_generator.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
