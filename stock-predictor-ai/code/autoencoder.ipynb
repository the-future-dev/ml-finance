{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 14:55:07.044366: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-05 14:55:07.071318: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-05 14:55:07.071354: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-05 14:55:07.071375: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-05 14:55:07.076867: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-05 14:55:07.757789: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from shared import read_dataset, plot_results,scatter_results, evaluate_price_predictions, mean_abs_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1248, 30, 21)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = \"2010-01-01\"\n",
    "end = \"2020-01-01\"\n",
    "target_column_name = 'Adj Close'\n",
    "model_path = '../models/lstm_model/price_generator.h5'\n",
    "\n",
    "GS = read_dataset('../data/GS.csv', start, end)\n",
    "\n",
    "GS['Close_Diff'] = GS['Close'].diff()\n",
    "GS.dropna(inplace=True) # initially NAN for moving averages\n",
    "\n",
    "cols = [target_column_name] + [ col for col in GS if col != target_column_name]\n",
    "target_column = list(GS.columns).index(target_column_name)\n",
    "data = GS[cols]\n",
    "\n",
    "# Define feature array and target array to train the model.\n",
    "data_array = np.array(data.values)\n",
    "target_array = np.array(data[target_column_name].values).reshape(-1, 1)\n",
    "\n",
    "# Normalize the data\n",
    "scaler_data = MinMaxScaler()\n",
    "scaler_data.fit(data_array)\n",
    "data_array = scaler_data.transform(data_array)\n",
    "\n",
    "scaler_target = MinMaxScaler()\n",
    "scaler_target.fit(target_array)\n",
    "target_array = scaler_target.transform(target_array)\n",
    "\n",
    "# Split the data\n",
    "train_size = int(len(data_array) * 0.50)\n",
    "evaluation_size = int(len(data_array) * 0.20) # no evaluation\n",
    "\n",
    "# Define data sequences\n",
    "def create_sequences(data, target, seq_length):\n",
    "    sequence_data = []\n",
    "    sequence_target = []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        sequence_data.append(data[i-seq_length:i])\n",
    "        sequence_target.append(target[i])\n",
    "    return np.array(sequence_data), np.array(sequence_target)\n",
    "\n",
    "SEQUENCE_LENGTH = 30\n",
    "data_sequences, target_sequences = create_sequences(data_array, target_array, SEQUENCE_LENGTH)\n",
    "\n",
    "# Shuffle the data\n",
    "shuffle_idxs = np.random.permutation(len(data_sequences))\n",
    "revert_idxs = np.argsort(shuffle_idxs)\n",
    "\n",
    "data_sequences = data_sequences[shuffle_idxs]\n",
    "target_sequences = target_sequences[shuffle_idxs]\n",
    "\n",
    "# Split the data into Train | Evaluation | Test datasets\n",
    "train_data, eval_data, test_data = data_sequences[:train_size], data_sequences[train_size:train_size+evaluation_size], data_sequences[train_size+evaluation_size:]\n",
    "train_target, eval_target, test_target = target_sequences[:train_size], target_sequences[train_size:train_size+evaluation_size], target_sequences[train_size+evaluation_size:]\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_just_autoencoded(input_shape):\n",
    "    \"\"\"LSTM model that takes encoded input\"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(64, input_shape=input_shape, activation='gelu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "def create_autoencoder(input_shape, encoding_dim):\n",
    "    input_seq = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    encoded = layers.LSTM(encoding_dim, activation='relu', return_sequences=True)(input_seq)\n",
    "    encoded = layers.LSTM(int(encoding_dim/2), activation='relu', return_sequences=False)(encoded)\n",
    "    encoded = layers.BatchNormalization()(encoded)\n",
    "    encoded = layers.Dropout(0.3) (encoded)\n",
    "    repeadted_encoding = layers.RepeatVector(input_shape[0])(encoded)  # repeat the encoding ro reshape it back into a sequence\n",
    "\n",
    "    # Decoder\n",
    "    decoded = layers.LSTM(int(encoding_dim/2), return_sequences=True, activation='relu')(repeadted_encoding)\n",
    "    decoded = layers.LSTM(encoding_dim, return_sequences=True, activation='relu')(decoded)\n",
    "    decoded = layers.BatchNormalization()(decoded)\n",
    "    decoded = layers.Dropout(0.3)(decoded)\n",
    "    decoded = layers.TimeDistributed(layers.Dense(input_shape[1]))(decoded)\n",
    "\n",
    "    autoencoder = keras.Model(input_seq, decoded)\n",
    "    encoder = keras.Model(input_seq, encoded)\n",
    "\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of encoding layer\n",
    "encoding_dim = 1024\n",
    "autoencoder_model_path = '../models/autoencoder/autoencoder.h5'\n",
    "\n",
    "input_shape = (train_data.shape[1], train_data.shape[2])\n",
    "autoencoder, encoder = create_autoencoder(input_shape, encoding_dim)\n",
    "# autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# autoencoder_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "#     autoencoder_model_path,\n",
    "#     monitor='val_loss',\n",
    "#     verbose=1,\n",
    "#     save_best_only=True,\n",
    "#     mode='min'\n",
    "# )\n",
    "# autoencoder.fit(train_data, train_data,\n",
    "#                 epochs=200,\n",
    "#                 batch_size=256,\n",
    "#                 shuffle=True,\n",
    "#                 validation_data=(eval_data, eval_data),\n",
    "#                 verbose=2,\n",
    "#                 callbacks=[autoencoder_checkpoint])\n",
    "\n",
    "autoencoder = load_model(autoencoder_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_data = encoder.predict(train_data)\n",
    "encoded_eval_data = encoder.predict(eval_data)\n",
    "encoded_test_data = encoder.predict(test_data)\n",
    "\n",
    "prediction_model_path = '../models/autoencoder/predictor.h5'\n",
    "\n",
    "encoded_input_shape = (encoded_train_data.shape[1],)\n",
    "prediction_model = build_model_just_autoencoded(encoded_input_shape)\n",
    "# prediction_model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "# predictor_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "#     prediction_model_path,\n",
    "#     monitor='val_loss',\n",
    "#     verbose=1,\n",
    "#     save_best_only=True,\n",
    "#     mode='min'\n",
    "# )\n",
    "\n",
    "# prediction_model.fit(\n",
    "#                     encoded_train_data,\n",
    "#                     train_target,\n",
    "#                     epochs=1000,\n",
    "#                     batch_size=256,\n",
    "#                     shuffle=True,\n",
    "#                     verbose=2,\n",
    "#                     validation_data=(encoded_eval_data, eval_target),\n",
    "#                     callbacks=[predictor_checkpoint],\n",
    "#                     )\n",
    "\n",
    "prediction_model = load_model(prediction_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_predicted_array = prediction_model.predict(encoded_train_data)\n",
    "price_predicted_array = scaler_target.inverse_transform(price_predicted_array)  # Denormalize predictions\n",
    "\n",
    "price_actual_array = scaler_target.inverse_transform(train_target.reshape(-1, 1)).flatten()  # Denormalize actual values\n",
    "\n",
    "# Plot the results\n",
    "plot_results(price_actual_array, price_predicted_array, target_column_name, title=\"Train Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_predicted_array = prediction_model.predict(encoded_eval_data)\n",
    "price_predicted_array = scaler_target.inverse_transform(price_predicted_array)  # Denormalize predictions\n",
    "\n",
    "price_actual_array = scaler_target.inverse_transform(eval_target.reshape(-1, 1)).flatten()  # Denormalize actual values\n",
    "\n",
    "# Plot the results\n",
    "plot_results(price_actual_array, price_predicted_array, target_column_name, title='Evaluation Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_predicted_array = prediction_model.predict(encoded_test_data)\n",
    "price_predicted_array = scaler_target.inverse_transform(price_predicted_array)  # Denormalize predictions\n",
    "\n",
    "price_actual_array = scaler_target.inverse_transform(test_target.reshape(-1, 1)).flatten()  # Denormalize actual values\n",
    "\n",
    "# Plot the results\n",
    "plot_results(price_actual_array, price_predicted_array, target_column_name, title='Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results obtained aren't good enough. By deleting the evaluation part they get better, but my deduction is that it's just thanks to overfitting and not thanks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_predicted_array = prediction_model.predict(encoded_test_data)\n",
    "price_predicted_array = scaler_target.inverse_transform(price_predicted_array)\n",
    "price_actual_array = scaler_target.inverse_transform(test_target.reshape(-1, 1)).flatten()\n",
    "\n",
    "evaluate_price_predictions(price_predicted_array, price_actual_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomposed_target = np.concatenate([train_target, eval_target, test_target])\n",
    "recomposed_target = scaler_target.inverse_transform(recomposed_target[revert_idxs])\n",
    "\n",
    "predicted_target = np.concatenate([train_target, eval_target, prediction_model.predict(encoded_test_data)])\n",
    "predicted_target = scaler_target.inverse_transform(predicted_target[revert_idxs])\n",
    "\n",
    "scatter_results(recomposed_target, predicted_target, target_column_name, title='All together')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Status: Work in Progress\n",
    "- non sta funzionando: neanche con shuffle data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
